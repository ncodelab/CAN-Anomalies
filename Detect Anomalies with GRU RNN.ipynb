{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect CAN Anomalies with RNN + GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use RNN Neural Network, with state and GRU (to learn reasonably, while have a state. Use logs as examples of non anomal sequences.\n",
    "As input, use embeddings of all CAN IDs, extended to all 2048 values.\n",
    "As output, use one-hot encoded probabilities.\n",
    "\n",
    "For anomalies prediction, check predicted probability of next CAN ID frame. Use `1 - p` to get probability of been an anomaly.\n",
    "\n",
    "Sum probablities of latest n frames from sequence and if value bigger than certain threshold, mark sequence as anomalous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible tweaks and questions:\n",
    "- Use only CAN frames found in logs. It will speed up learning. But how to extend if we need to add new CAN IDs?\n",
    "- Use squared probabilities `(1 - p)^2`. Just check it. Also check `1/p` and `1/p^2`\n",
    "- Size of sequences. It can vary, like: 32-128. Bigger is better (should be more accurate). Smaller is quicker (not 100% sure if it's true to learn, but 100% quicker to predict). Should measure that.\n",
    "- Epsilon parameter, which will indicate that certain places in logs are anomalous.\n",
    "- Do we need examples of anomalous logs in training set? Does it increase accuracy?\n",
    "- Do we need embeddings for input? Or just one-hot encode them? Does it affect accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional benefits\n",
    "- We can compare length between embedding vectors for CAN IDs and find similar ones.\n",
    "- We can find the most common and the least common IDs.\n",
    "- We can find the most common and the least common group of IDs (sequences, but w/o certain start point)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training, validation and test sets\n",
    "- Training set\n",
    "Take log examples,take only normal logs. Take random parts, including starts and finishes of log files. Cut them into sequences and use them to train.\n",
    "- Validation and test sets\n",
    "Take log examples as mentioned in **Training Set**, but add examples of abnormal sequences too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from importlib import reload\n",
    "import utils; reload(utils)\n",
    "from utils import *\n",
    "from __future__ import division, print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# size of hidden state\n",
    "n_hidden = 256\n",
    "\n",
    "# sequence length\n",
    "seq_len = 64\n",
    "\n",
    "# length of CAN IDs vocabulary\n",
    "vocab_size = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "        GRU(n_hidden, return_sequences=True, input_shape=(seq_len, vocab_size),\n",
    "                  activation='relu', inner_init='identity'),\n",
    "        TimeDistributed(Dense(vocab_size, activation='softmax')),\n",
    "    ])\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oh_x_rnn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-d5043d7b19d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moh_x_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moh_y_rnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'oh_x_rnn' is not defined"
     ]
    }
   ],
   "source": [
    "model.fit(oh_x_rnn, oh_y_rnn, batch_size=64, nb_epoch=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
